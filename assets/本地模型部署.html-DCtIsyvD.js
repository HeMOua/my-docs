import{_ as e}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as r,a as n,o as a}from"./app-BDhRxcGH.js";const d={};function o(l,t){return a(),r("div",null,t[0]||(t[0]=[n('<h1 id="本地模型部署" tabindex="-1"><a class="header-anchor" href="#本地模型部署"><span>本地模型部署</span></a></h1><h2 id="常见框架" tabindex="-1"><a class="header-anchor" href="#常见框架"><span>常见框架</span></a></h2><p>本地模型部署框架如下：</p><table><thead><tr><th>模型部署框架</th><th>Xinference</th><th>LocalAI</th><th>Ollama</th><th>FastChat</th></tr></thead><tbody><tr><td>OpenAI API 接口对齐</td><td>✅</td><td>✅</td><td>✅</td><td>✅</td></tr><tr><td>加速推理引擎</td><td>GPTQ, GGML, vLLM, TensorRT, mlx</td><td>GPTQ, GGML, vLLM, TensorRT</td><td>GGUF, GGML</td><td>vLLM</td></tr><tr><td>接入模型类型</td><td>LLM, Embedding, Rerank, Text-to-Image, Vision, Audio</td><td>LLM, Embedding, Rerank, Text-to-Image, Vision, Audio</td><td>LLM, Text-to-Image, Vision</td><td>LLM, Vision</td></tr><tr><td>Function Call</td><td>✅</td><td>✅</td><td>✅</td><td>/</td></tr><tr><td>更多平台支持(CPU, Metal)</td><td>✅</td><td>✅</td><td>✅</td><td>✅</td></tr><tr><td>异构</td><td>✅</td><td>✅</td><td>/</td><td>/</td></tr><tr><td>集群</td><td>✅</td><td>✅</td><td>/</td><td>/</td></tr><tr><td>操作文档链接</td><td><a href="https://inference.readthedocs.io/zh-cn/latest/models/builtin/index.html" target="_blank" rel="noopener noreferrer">Xinference 文档</a></td><td><a href="https://localai.io/model-compatibility/" target="_blank" rel="noopener noreferrer">LocalAI 文档</a></td><td><a href="https://github.com/ollama/ollama?tab=readme-ov-file#model-library" target="_blank" rel="noopener noreferrer">Ollama 文档</a></td><td><a href="https://github.com/lm-sys/FastChat#install" target="_blank" rel="noopener noreferrer">FastChat 文档</a></td></tr><tr><td>可用模型</td><td><a href="https://inference.readthedocs.io/en/latest/models/builtin/index.html" target="_blank" rel="noopener noreferrer">Xinference 已支持模型</a></td><td><a href="https://localai.io/model-compatibility/#/" target="_blank" rel="noopener noreferrer">LocalAI 已支持模型</a></td><td><a href="https://ollama.com/library#/" target="_blank" rel="noopener noreferrer">Ollama 已支持模型</a></td><td><a href="https://github.com/lm-sys/FastChat/blob/main/docs/model_support.md" target="_blank" rel="noopener noreferrer">FastChat 已支持模型</a></td></tr></tbody></table><h2 id="加速推理引擎" tabindex="-1"><a class="header-anchor" href="#加速推理引擎"><span>加速推理引擎</span></a></h2><ol><li><strong>GPTQ (Quantized Transformer Quantization)</strong></li></ol><p>GPTQ是一种量化方法，主要用于将大型Transformer模型（例如GPT模型）进行参数量化，从而减少内存占用和加速推理。GPTQ通常通过<mark>把模型权重从高精度（如FP32）减少到较低精度（如INT8）</mark>，来在推理过程中保持相似的精度表现，但显著降低了计算和存储成本。</p><ol start="2"><li><strong>GGML (Geospatial Gaussian Mixture Library)</strong></li></ol><p>GGML是一个轻量化的库，专门用于在CPU上运行大型语言模型，通常应用于推理阶段。它基于<mark>量化权重</mark>来使得模型在低端硬件上运行，从而使得复杂模型在资源受限的环境中部署成为可能。GGML支持对模型进行不同程度的量化以优化内存占用，并且<mark>适合嵌入式设备和无GPU环境</mark>。</p><ol start="3"><li><strong>vLLM (virtual LLM)</strong></li></ol><p>vLLM是一种优化大型语言模型推理速度的技术，通常<mark>结合了内存管理和计算图优化来实现高效推理</mark>。vLLM通过管理内存的动态分配和释放，减少不必要的计算，以及使用缓存技术以加速推理，特别<mark>适合在有限资源的环境下执行高效的大规模模型推理</mark>。</p><ol start="4"><li><strong>TensorRT</strong></li></ol><p>TensorRT是NVIDIA开发的一个深度学习推理优化库，专为在NVIDIA GPU上加速深度学习模型的推理性能。它通过一系列优化，例如内核融合、精度量化（FP16/INT8），以及图优化，将训练好的模型转换为高效的推理引擎。<mark>TensorRT适用于需要低延迟和高吞吐量的场景，特别是在NVIDIA GPU硬件上进行推理部署</mark>。</p><ol start="5"><li><strong>MLX (Model Loader for Exchange)</strong></li></ol><p>MLX通常指一种用于跨平台和跨框架加载深度学习模型的工具或协议。它的目标是使不同框架的模型更容易在不同环境中部署和使用。通过标准化模型的输入/输出和格式，<mark>MLX减少了在不同平台之间迁移模型的复杂性</mark>，使得开发者可以方便地利用不同硬件和软件平台的优势。</p><ol start="6"><li><strong>GGUF (Grok GGML Unified Format)</strong></li></ol><p>GGUF是GGML模型的一种新格式，专门设计用于在不同平台上实现统一的模型加载和推理体验。它的目标是解决模型格式的兼容性问题，使得使用者可以<mark>跨平台部署相同模型</mark>，而无需对模型格式进行大量的手动调整。GGUF集成了一些优化，特别适合在轻量级环境下的推理任务。</p>',17)]))}const h=e(d,[["render",o],["__file","本地模型部署.html.vue"]]),m=JSON.parse('{"path":"/knowledge/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/%E6%9C%AC%E5%9C%B0%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/%E6%9C%AC%E5%9C%B0%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2.html","title":"本地模型部署","lang":"zh-CN","frontmatter":{"description":"本地模型部署 常见框架 本地模型部署框架如下： 加速推理引擎 GPTQ (Quantized Transformer Quantization) GPTQ是一种量化方法，主要用于将大型Transformer模型（例如GPT模型）进行参数量化，从而减少内存占用和加速推理。GPTQ通常通过把模型权重从高精度（如FP32）减少到较低精度（如INT8），来在推...","head":[["meta",{"property":"og:url","content":"https://jishuzhaix.cn/knowledge/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/%E6%9C%AC%E5%9C%B0%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/%E6%9C%AC%E5%9C%B0%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2.html"}],["meta",{"property":"og:site_name","content":"技数斋"}],["meta",{"property":"og:title","content":"本地模型部署"}],["meta",{"property":"og:description","content":"本地模型部署 常见框架 本地模型部署框架如下： 加速推理引擎 GPTQ (Quantized Transformer Quantization) GPTQ是一种量化方法，主要用于将大型Transformer模型（例如GPT模型）进行参数量化，从而减少内存占用和加速推理。GPTQ通常通过把模型权重从高精度（如FP32）减少到较低精度（如INT8），来在推..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-02-17T14:21:20.000Z"}],["meta",{"property":"article:modified_time","content":"2025-02-17T14:21:20.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"本地模型部署\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2025-02-17T14:21:20.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"贺墨于\\",\\"url\\":\\"https://jishuzhaix.cn\\"}]}"]]},"headers":[{"level":2,"title":"常见框架","slug":"常见框架","link":"#常见框架","children":[]},{"level":2,"title":"加速推理引擎","slug":"加速推理引擎","link":"#加速推理引擎","children":[]}],"git":{"createdTime":1739802080000,"updatedTime":1739802080000,"contributors":[{"name":"HeMOu","username":"HeMOu","email":"fangqichenchao@163.com","commits":1,"url":"https://github.com/HeMOu"}]},"readingTime":{"minutes":2.86,"words":858},"filePathRelative":"knowledge/人工智能/大语言模型/本地模型部署/本地模型部署.md","localizedDate":"2025年2月17日","excerpt":"\\n<h2>常见框架</h2>\\n<p>本地模型部署框架如下：</p>\\n<table>\\n<thead>\\n<tr>\\n<th>模型部署框架</th>\\n<th>Xinference</th>\\n<th>LocalAI</th>\\n<th>Ollama</th>\\n<th>FastChat</th>\\n</tr>\\n</thead>\\n<tbody>\\n<tr>\\n<td>OpenAI API 接口对齐</td>\\n<td>✅</td>\\n<td>✅</td>\\n<td>✅</td>\\n<td>✅</td>\\n</tr>\\n<tr>\\n<td>加速推理引擎</td>\\n<td>GPTQ, GGML, vLLM, TensorRT, mlx</td>\\n<td>GPTQ, GGML, vLLM, TensorRT</td>\\n<td>GGUF, GGML</td>\\n<td>vLLM</td>\\n</tr>\\n<tr>\\n<td>接入模型类型</td>\\n<td>LLM, Embedding, Rerank, Text-to-Image, Vision, Audio</td>\\n<td>LLM, Embedding, Rerank, Text-to-Image, Vision, Audio</td>\\n<td>LLM, Text-to-Image, Vision</td>\\n<td>LLM, Vision</td>\\n</tr>\\n<tr>\\n<td>Function Call</td>\\n<td>✅</td>\\n<td>✅</td>\\n<td>✅</td>\\n<td>/</td>\\n</tr>\\n<tr>\\n<td>更多平台支持(CPU, Metal)</td>\\n<td>✅</td>\\n<td>✅</td>\\n<td>✅</td>\\n<td>✅</td>\\n</tr>\\n<tr>\\n<td>异构</td>\\n<td>✅</td>\\n<td>✅</td>\\n<td>/</td>\\n<td>/</td>\\n</tr>\\n<tr>\\n<td>集群</td>\\n<td>✅</td>\\n<td>✅</td>\\n<td>/</td>\\n<td>/</td>\\n</tr>\\n<tr>\\n<td>操作文档链接</td>\\n<td><a href=\\"https://inference.readthedocs.io/zh-cn/latest/models/builtin/index.html\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">Xinference 文档</a></td>\\n<td><a href=\\"https://localai.io/model-compatibility/\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">LocalAI 文档</a></td>\\n<td><a href=\\"https://github.com/ollama/ollama?tab=readme-ov-file#model-library\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">Ollama 文档</a></td>\\n<td><a href=\\"https://github.com/lm-sys/FastChat#install\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">FastChat 文档</a></td>\\n</tr>\\n<tr>\\n<td>可用模型</td>\\n<td><a href=\\"https://inference.readthedocs.io/en/latest/models/builtin/index.html\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">Xinference 已支持模型</a></td>\\n<td><a href=\\"https://localai.io/model-compatibility/#/\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">LocalAI 已支持模型</a></td>\\n<td><a href=\\"https://ollama.com/library#/\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">Ollama 已支持模型</a></td>\\n<td><a href=\\"https://github.com/lm-sys/FastChat/blob/main/docs/model_support.md\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">FastChat 已支持模型</a></td>\\n</tr>\\n</tbody>\\n</table>","autoDesc":true}');export{h as comp,m as data};
